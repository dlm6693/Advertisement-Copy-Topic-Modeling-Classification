{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dlm66\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\Users\\dlm66\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dlm66\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>country</th>\n",
       "      <th>medium</th>\n",
       "      <th>headline</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tzabar</td>\n",
       "      <td>Israel</td>\n",
       "      <td>Print</td>\n",
       "      <td>roger waters live in paris 3 nights  concert €...</td>\n",
       "      <td>Transport &amp; Tourism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Infiniti</td>\n",
       "      <td>United States</td>\n",
       "      <td>Print</td>\n",
       "      <td>accelerating the future</td>\n",
       "      <td>Automotive &amp; Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Toyota</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Print</td>\n",
       "      <td>official ride of the spinfest amateur dj awards</td>\n",
       "      <td>Automotive &amp; Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Friends of the Earth</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Print</td>\n",
       "      <td>travelling fruits cause pollution think global...</td>\n",
       "      <td>Public interest &amp; Non-profit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Max Factor</td>\n",
       "      <td>Colombia</td>\n",
       "      <td>Print</td>\n",
       "      <td>the art of beauty</td>\n",
       "      <td>Beauty &amp; Health</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  brand        country medium  \\\n",
       "0                Tzabar         Israel  Print   \n",
       "1              Infiniti  United States  Print   \n",
       "2                Toyota         Canada  Print   \n",
       "3  Friends of the Earth        Germany  Print   \n",
       "4            Max Factor       Colombia  Print   \n",
       "\n",
       "                                            headline  \\\n",
       "0  roger waters live in paris 3 nights  concert €...   \n",
       "1                            accelerating the future   \n",
       "2    official ride of the spinfest amateur dj awards   \n",
       "3  travelling fruits cause pollution think global...   \n",
       "4                                  the art of beauty   \n",
       "\n",
       "                       industry  \n",
       "0           Transport & Tourism  \n",
       "1         Automotive & Services  \n",
       "2         Automotive & Services  \n",
       "3  Public interest & Non-profit  \n",
       "4               Beauty & Health  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headlines = pd.read_csv('headlines_df.csv')\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = headlines.headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return SnowballStemmer('english').stem(WordNetLemmatizer().lemmatize(text))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roger waters live in paris 3 nights  concert €699 monday 30 may 2011'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['roger', 'waters', 'live', 'in', 'paris', '3', 'nights', '', 'concert', '€699', 'monday', '30', 'may', '2011']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['roger', 'water', 'live', 'pari', 'night', 'concert', 'monday']\n"
     ]
    }
   ],
   "source": [
    "sample = text[0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [roger, water, live, pari, night, concert, mon...\n",
       "1                                     [acceler, futur]\n",
       "2             [offici, ride, spinfest, amateur, award]\n",
       "3    [travel, fruit, caus, pollut, think, global, l...\n",
       "4                                             [beauti]\n",
       "5                   [make, monday, tast, like, friday]\n",
       "6                                    [seatbelt, excus]\n",
       "7                                               [tast]\n",
       "8       [offici, licens, gala, poster, game, lotusiad]\n",
       "9                                       [soft, strong]\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = text.map(preprocess)\n",
    "processed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 1),\n",
       " (27, 1),\n",
       " (97, 1),\n",
       " (129, 1),\n",
       " (614, 1),\n",
       " (669, 1),\n",
       " (745, 1),\n",
       " (978, 1),\n",
       " (979, 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed]\n",
    "bow_corpus[510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
